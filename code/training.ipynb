{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cd5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ardca import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0586c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_path = \"../data/PF00014_mgap6.fasta.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842444cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MSA data...\n",
      "MSA shape: (13600, 51)\n",
      "Effective sequences: 4208.5\n",
      "Sequence length: 51\n",
      "Alphabet size: 21\n",
      "Training ArDCA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]/Users/tommasoferracina/bocconi/thesis/ArDCA-Python/code/.arvenv/lib/python3.13/site-packages/torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  loss = float(closure())\n",
      "Training:   0%|          | 1/200 [00:37<2:05:08, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss=40937.9783, Val Loss=105590.5938, Val Perplexity=2.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–         | 8/200 [05:32<2:12:49, 41.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsa_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_h\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_J\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m13\u001b[39;49m\u001b[43m-\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gap_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentity_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bocconi/thesis/ArDCA-Python/code/ardca.py:360\u001b[39m, in \u001b[36mmain_training_pipeline\u001b[39m\u001b[34m(fasta_file, lambda_h, lambda_J, max_gap_fraction, max_col_gap_fraction, identity_thresh, val_frac, max_iters, seed, device)\u001b[39m\n\u001b[32m    357\u001b[39m model = ArDCA(L=msa_data.L, q=msa_data.q, lambda_h=lambda_h, lambda_J=lambda_J)\n\u001b[32m    359\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining ArDCA model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m model, history = \u001b[43mtrain_ardca\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsa_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsa_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# Evaluate final model\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bocconi/thesis/ArDCA-Python/code/ardca.py:227\u001b[39m, in \u001b[36mtrain_ardca\u001b[39m\u001b[34m(model, msa_data, model_params, device)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# Step with closure for LBFGS\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Compute loss for logging (after optimization)\u001b[39;00m\n\u001b[32m    230\u001b[39m loss, info = model.loss(\n\u001b[32m    231\u001b[39m     batch_seqs, \n\u001b[32m    232\u001b[39m     batch_weights,\n\u001b[32m    233\u001b[39m     lambda_J=model_params.lambda_J,\n\u001b[32m    234\u001b[39m     lambda_h=model_params.lambda_h\n\u001b[32m    235\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bocconi/thesis/ArDCA-Python/code/.arvenv/lib/python3.13/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bocconi/thesis/ArDCA-Python/code/.arvenv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bocconi/thesis/ArDCA-Python/code/.arvenv/lib/python3.13/site-packages/torch/optim/lbfgs.py:401\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old - \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m):\n\u001b[32m    400\u001b[39m     al[i] = old_stps[i].dot(q) * ro[i]\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mal\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n\u001b[32m    405\u001b[39m d = r = torch.mul(q, H_diag)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "main_training_pipeline(msa_path, lambda_h=1e-6, lambda_J=13-4, max_gap_fraction=1.0, identity_thresh=0.8, val_frac=0.2, max_iters=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arvenv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
